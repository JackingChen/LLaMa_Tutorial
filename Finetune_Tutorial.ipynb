{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "讀進library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "from llama_recipes.utils import get_preprocessed_dataset\n",
    "from llama_recipes.configs.datasets import samsum_dataset\n",
    "\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import TrainerCallback\n",
    "from contextlib import nullcontext\n",
    "from transformers import default_data_collator, Trainer, TrainingArguments\n",
    "import datasets\n",
    "\n",
    "from llama_recipes.datasets.utils import Concatenator\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constant 參數設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arg():\n",
    "    # 創建 ArgumentParser 物件\n",
    "    parser = argparse.ArgumentParser(description=\"這是一個簡單的命令行程式\")\n",
    "\n",
    "    # 添加命令行參數\n",
    "    parser.add_argument('--base_mdl', help='從哪個model開始tune', default=\"/mnt/External/Seagate/FedASR/LLaMa2/7B_hf/\")\n",
    "    parser.add_argument('--output_dir',help='', default=\"tmp/Tuned-MeduAD\")\n",
    "    parser.add_argument('--train_eval_split_num', help='eval data從地幾個號碼開始缺', default=200)\n",
    "    parser.add_argument('--datasetfile_path',help='', default=\"./train-00000-of-00001-4401d00b2bdd1863.parquet\")\n",
    "\n",
    "    # 解析命令行參數\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # 在這裡添加實際的程式邏輯，例如複製檔案或處理資料\n",
    "    return args\n",
    "\n",
    "args=parse_arg()\n",
    "\n",
    "model_id = args.base_mdl\n",
    "output_dir = args.output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "讀入模型\n",
    "\n",
    "可能要一兩分鐘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "model =LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map=\"auto\", torch_dtype=torch.float16,\\\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "讀入資料庫\n",
    "\n",
    "這個例子選用的是 MeQuAD 資料庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parquet_file_path = \"C:/Users/iec120955/Downloads/train-00000-of-00001-4401d00b2bdd1863.parquet\"\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# 使用pandas读取Parquet文件\n",
    "df = pd.read_parquet(args.datasetfile_path)\n",
    "MeQuADdataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "處理MeQuAD資料庫\n",
    "* 設定prompt template\n",
    "* apply_prompt_template: 把每個sample都塞進prompt template\n",
    "* 把prompt用tokenizer轉成數字\n",
    "* chunk: 把過長的切成下一個sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_MeQuAD_dataset(MeQuADdataset, tokenizer, split='train'):\n",
    "    prompt = (\n",
    "        f\"Answer this question:\\n{{question}}\\n---\\nAnswer:\\n{{answer}}{{eos_token}}\"\n",
    "    )\n",
    "\n",
    "    def apply_prompt_template(sample):\n",
    "        return {\n",
    "            \"text\": prompt.format(\n",
    "                question=sample[\"Questions\"],\n",
    "                answer=sample[\"Answers\"],\n",
    "                eos_token=tokenizer.eos_token,\n",
    "            )\n",
    "        }\n",
    "\n",
    "    MeQuADdataset = MeQuADdataset.map(apply_prompt_template, remove_columns=list(MeQuADdataset.features))\n",
    "        \n",
    "    MeQuADdataset = MeQuADdataset.map(\n",
    "        lambda sample: tokenizer(sample[\"text\"]),\n",
    "        batched=True,\n",
    "        remove_columns=list(MeQuADdataset.features),\n",
    "    ).map(Concatenator(), batched=True)\n",
    "    return MeQuADdataset\n",
    "\n",
    "MeQuAD_dataset = process_MeQuAD_dataset(MeQuADdataset, tokenizer, 'train')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把資料庫切成Train跟test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval_split_num=200\n",
    "\n",
    "train_dataset = MeQuAD_dataset.select(\n",
    "    range(len(MeQuAD_dataset))[:train_eval_split_num]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finutune本身\n",
    "\n",
    "Finetune一律都用這些固定參數就好了，先盡量不要動這一塊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.train()\n",
    "def create_peft_config(model):\n",
    "    from peft import (\n",
    "        get_peft_model,\n",
    "        LoraConfig,\n",
    "        TaskType,\n",
    "        prepare_model_for_int8_training,\n",
    "    )\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules = [\"q_proj\", \"v_proj\"]\n",
    "    )\n",
    "\n",
    "    # prepare int-8 model for training\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model, peft_config\n",
    "\n",
    "# create peft config\n",
    "model, lora_config = create_peft_config(model)\n",
    "\n",
    "\n",
    "enable_profiler = False\n",
    "\n",
    "\n",
    "config = {\n",
    "    'lora_config': lora_config,\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_train_epochs': 1,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'per_device_train_batch_size': 2,\n",
    "    'gradient_checkpointing': False,\n",
    "}\n",
    "\n",
    "# Set up profiler\n",
    "if enable_profiler:\n",
    "    wait, warmup, active, repeat = 1, 1, 2, 1\n",
    "    total_steps = (wait + warmup + active) * (1 + repeat)\n",
    "    schedule =  torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=repeat)\n",
    "    profiler = torch.profiler.profile(\n",
    "        schedule=schedule,\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(f\"{output_dir}/logs/tensorboard\"),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True)\n",
    "    \n",
    "    class ProfilerCallback(TrainerCallback):\n",
    "        def __init__(self, profiler):\n",
    "            self.profiler = profiler\n",
    "            \n",
    "        def on_step_end(self, *args, **kwargs):\n",
    "            self.profiler.step()\n",
    "\n",
    "    profiler_callback = ProfilerCallback(profiler)\n",
    "else:\n",
    "    profiler = nullcontext()\n",
    "\n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    bf16=True,  # Use BF16 if available\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    max_steps=total_steps if enable_profiler else -1,\n",
    "    **{k:v for k,v in config.items() if k != 'lora_config'}\n",
    ")\n",
    "\n",
    "with profiler:\n",
    "    # Create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=default_data_collator,\n",
    "        callbacks=[profiler_callback] if enable_profiler else [],\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到這邊就已經finetune好了，model就是finetune的模型\n",
    "\n",
    "接下來要儲存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
